{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnQbA7eZKlzQLDmev1JZdp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilloufsky/hw3/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpoLFz8U8ief",
        "outputId": "a421a66b-cee8-475e-ee5c-d2eff8160f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbEjUM5B9Ic6",
        "outputId": "67ebf410-ebb3-4fd6-922a-52fe701c5b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7q0tLDZ9Nmm",
        "outputId": "46d6f3a2-9dd6-4436-9b41-8d288138fad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC0zaj6M9UIc",
        "outputId": "4b0435be-fb18-456b-e0df-663ec597333d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2cxyjju9crv",
        "outputId": "d6d1c2e5-f27a-446e-c484-e660b592e4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFOfYZx59yob",
        "outputId": "5759f0f5-ffa2-4209-c270-af1df5fd1928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIwEqfvSDGYV",
        "outputId": "f063dba2-2a35-4aba-a179-3765ff3a85ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y7fvwNbDhH9",
        "outputId": "08682089-4f22-45bf-e0d3-3a8341b6a180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# نمایش لیست فایل‌ها و دایرکتوری‌های موجود\n",
        "print(os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI4kXxlCEEcu",
        "outputId": "5d942221-4c24-461e-ae77-d3a0077e128e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'CISI.ALL', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# import os\n",
        "\n",
        "# # دانلود دیتاست و دریافت محتوای فایل\n",
        "# cisi_content = \"\"\"\n",
        "# .I 1\n",
        "# .T\n",
        "# Sample Document One\n",
        "# .X\n",
        "# Here is some text for the first sample document.\n",
        "# .I 2\n",
        "# .T\n",
        "# Sample Document Two\n",
        "# .X\n",
        "# This is the text for the second sample document.\n",
        "# \"\"\"\n",
        "\n",
        "# # ذخیره محتوای فایل در یک فایل موقت\n",
        "# with open(\"CISI.ALL\", \"w\") as f:\n",
        "#     f.write(cisi_content)\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # خواندن محتوای فایل\n",
        "# with open(\"CISI.ALL\", \"r\") as f:\n",
        "#     documents = f.read().split(\".I\")[1:]\n",
        "\n",
        "# # پیش‌پردازش و stem کردن متنها و حساب tf-idf انها\n",
        "# vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "# tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# # ساخت DataFrame از ماتریس tf-idf\n",
        "# df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# # ذخیره DataFrame در فایل Excel\n",
        "# df.to_excel(\"tf_idf_results.xlsx\", index=False)\n",
        "# print(\"Results saved to tf_idf_results.xlsx\")\n",
        "\n",
        "# # حذف فایل موقت\n",
        "# os.remove(\"CISI.ALL\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJXtBJmAEM_6",
        "outputId": "306e8326-e899-4807-dc7d-c19e3b452e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_idf_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # پیش‌پردازش و stem کردن متنها و حساب tf-idf انها\n",
        "# vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "# tfidf_matrix = vectorizer.fit_transform([cisi_content])\n",
        "\n",
        "# # ساخت DataFrame از ماتریس tf-idf\n",
        "# df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# # ذخیره DataFrame در فایل Excel\n",
        "# df.to_excel(\"tf_idf_results.xlsx\", index=False)\n",
        "# print(\"Results saved to tf_idf_results.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C96ypUuYHeQL",
        "outputId": "54dede68-6fd2-452b-d44a-8f8d01b5d62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_idf_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # پیش‌پردازش و stem کردن متنها و حساب tf-idf انها\n",
        "# vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "# tfidf_matrix = vectorizer.fit_transform([cisi_content])\n",
        "\n",
        "# # ساخت DataFrame از ماتریس tf-idf\n",
        "# df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# # حساب tf برای هر کلمه\n",
        "# tf = df.sum(axis=0)\n",
        "\n",
        "# # حساب idf برای هر کلمه\n",
        "# idf = df[df > 0].count(axis=0)\n",
        "# idf = idf.apply(lambda x: len(df) / x)\n",
        "\n",
        "# # ذخیره tf و idf در فایل Excel\n",
        "# df_tf_idf = pd.DataFrame({\"Term\": vectorizer.get_feature_names_out(), \"TF\": tf, \"IDF\": idf})\n",
        "# df_tf_idf.to_excel(\"tf_idf_results.xlsx\", index=False)\n",
        "# print(\"Results saved to tf_idf_results.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbUEI6cYskWq",
        "outputId": "7c2e218b-e1b4-44d9-9ec1-593a79ae73eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_idf_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # پیش‌پردازش و stem کردن متنها و حساب tf-idf انها\n",
        "# vectorizer = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "# tfidf_matrix = vectorizer.fit_transform([cisi_content])\n",
        "\n",
        "# # ساخت DataFrame از ماتریس tf-idf\n",
        "# df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# # حساب tf برای هر کلمه\n",
        "# tf = df.sum(axis=0)\n",
        "\n",
        "# # حساب idf برای هر کلمه\n",
        "# idf = df[df > 0].count(axis=0)\n",
        "# idf = idf.apply(lambda x: len(df) / x)\n",
        "\n",
        "# # محاسبه tf-idf برای هر کلمه\n",
        "# tf_idf = df.multiply(idf, axis=1)\n",
        "\n",
        "# # ذخیره tf، idf و tf-idf در فایل Excel\n",
        "# df_tf_idf = pd.DataFrame({\"Term\": vectorizer.get_feature_names_out(), \"TF\": tf, \"IDF\": idf, \"TF-IDF\": tf_idf.sum(axis=0)})\n",
        "# df_tf_idf.to_excel(\"tf_idf_results.xlsx\", index=False)\n",
        "# print(\"Results saved to tf_idf_results.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQJIn_qItNTV",
        "outputId": "81082e91-3741-45f8-b6ef-fc53064d2443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_idf_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# import os\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # پیش‌پردازش و stem کردن متنها و حساب tf انها\n",
        "# vectorizer_tf = TfidfVectorizer(preprocessor=preprocess_text, use_idf=False)\n",
        "# tf_matrix = vectorizer_tf.fit_transform([cisi_content])\n",
        "\n",
        "# # حساب idf برای هر کلمه\n",
        "# vectorizer_idf = TfidfVectorizer(preprocessor=preprocess_text, use_idf=True)\n",
        "# idf_matrix = vectorizer_idf.fit_transform([cisi_content])\n",
        "# idf = idf_matrix.toarray()[0]\n",
        "\n",
        "# # محاسبه tf-idf برای هر کلمه\n",
        "# vectorizer_tfidf = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "# tfidf_matrix = vectorizer_tfidf.fit_transform([cisi_content])\n",
        "\n",
        "# # استخراج نام کلمات\n",
        "# terms = vectorizer_tf.get_feature_names_out()\n",
        "\n",
        "# # حساب tf برای هر کلمه\n",
        "# tf = tf_matrix.toarray()[0]\n",
        "\n",
        "# # ساخت DataFrame برای tf\n",
        "# df_tf = pd.DataFrame({\"Term\": terms, \"TF\": tf})\n",
        "\n",
        "# # ساخت DataFrame برای idf\n",
        "# df_idf = pd.DataFrame({\"Term\": terms, \"IDF\": idf})\n",
        "\n",
        "# # ساخت DataFrame برای tf-idf\n",
        "# df_tfidf = pd.DataFrame({\"Term\": terms, \"TF-IDF\": tfidf_matrix.toarray()[0]})\n",
        "\n",
        "# # ذخیره DataFrame در فایل‌های Excel\n",
        "# df_tf.to_excel(\"tf_results.xlsx\", index=False)\n",
        "# df_idf.to_excel(\"idf_results.xlsx\", index=False)\n",
        "# df_tfidf.to_excel(\"tf_idf_results.xlsx\", index=False)\n",
        "\n",
        "# print(\"Results saved to tf_results.xlsx, idf_results.xlsx, and tf_idf_results.xlsx\")\n",
        "\n",
        "# # حذف فایل موقت\n",
        "# os.remove(\"CISI.ALL\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn3qIoY-wD46",
        "outputId": "87d4fa3f-4466-41be-e954-2a4806625ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_results.xlsx, idf_results.xlsx, and tf_idf_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# import os\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # پیش‌پردازش و stem کردن متنها و حساب tf انها\n",
        "# vectorizer_tf = TfidfVectorizer(preprocessor=preprocess_text, use_idf=False)\n",
        "# tf_matrix = vectorizer_tf.fit_transform([cisi_content])\n",
        "\n",
        "# # حساب idf برای هر کلمه\n",
        "# vectorizer_idf = TfidfVectorizer(preprocessor=preprocess_text, use_idf=True)\n",
        "# idf_matrix = vectorizer_idf.fit_transform([cisi_content])\n",
        "# idf = idf_matrix.toarray()[0]\n",
        "\n",
        "# # محاسبه tf-idf برای هر کلمه\n",
        "# vectorizer_tfidf = TfidfVectorizer(preprocessor=preprocess_text)\n",
        "# tfidf_matrix = vectorizer_tfidf.fit_transform([cisi_content])\n",
        "\n",
        "# # استخراج نام کلمات\n",
        "# terms = vectorizer_tf.get_feature_names_out()\n",
        "\n",
        "# # حساب tf برای هر کلمه\n",
        "# tf = tf_matrix.toarray()[0]\n",
        "\n",
        "# # ساخت DataFrame برای tf\n",
        "# df_tf = pd.DataFrame({\"Term\": terms, \"TF\": tf})\n",
        "\n",
        "# # ساخت DataFrame برای idf\n",
        "# df_idf = pd.DataFrame({\"Term\": terms, \"IDF\": idf})\n",
        "\n",
        "# # ساخت DataFrame برای tf-idf\n",
        "# df_tfidf = pd.DataFrame({\"Term\": terms, \"TF-IDF\": tfidf_matrix.toarray()[0]})\n",
        "\n",
        "# # ایجاد فایل اکسل جدید\n",
        "# with pd.ExcelWriter('tf_idf_results.xlsx') as writer:\n",
        "#     df_tf.to_excel(writer, sheet_name='TF', index=False)\n",
        "#     df_idf.to_excel(writer, sheet_name='IDF', index=False)\n",
        "#     df_tfidf.to_excel(writer, sheet_name='TF-IDF', index=False)\n",
        "\n",
        "# print(\"Results saved to tf_idf_results.xlsx\")\n",
        "\n",
        "# # حذف فایل موقت\n",
        "# os.remove(\"CISI.ALL\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqmlj1vE5ah9",
        "outputId": "bba2b211-d50b-4b83-9f5d-0e15a344bba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_idf_results.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# import os\n",
        "# import re\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # تابعی برای محاسبه tf و idf برای هر داکیومنت\n",
        "# def calculate_tf_idf(documents):\n",
        "#     # تبدیل متن به ماتریس تعداد کلمات (Term Frequency)\n",
        "#     count_vectorizer = CountVectorizer(preprocessor=preprocess_text)\n",
        "#     term_counts = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "#     # محاسبه مقدار tf-idf\n",
        "#     tfidf_transformer = TfidfTransformer()\n",
        "#     tfidf_matrix = tfidf_transformer.fit_transform(term_counts)\n",
        "\n",
        "#     return tfidf_matrix, count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # جدا کردن داکیومنت‌ها بر اساس I هر داکیومنت\n",
        "# documents = cisi_content.split(\".I\")[1:]\n",
        "\n",
        "# # لیستی برای ذخیره tf-idf هر داکیومنت\n",
        "# dfs_tf = []\n",
        "# dfs_idf = []\n",
        "\n",
        "# # محاسبه tf و idf برای هر داکیومنت جداگانه\n",
        "# for doc in documents:\n",
        "#     # شناسایی الگوی .T در داکیومنت\n",
        "#     pattern = re.compile(r\"\\.T\", re.IGNORECASE)\n",
        "#     match = pattern.search(doc)\n",
        "\n",
        "#     if match:\n",
        "#         # اگر الگو پیدا شود، دو بخش جداگانه از داکیومنت استخراج می‌شود\n",
        "#         doc_id, doc_content = doc.split(\".T\")[0], doc.split(\".T\")[1].strip()\n",
        "#     else:\n",
        "#         # اگر الگو پیدا نشود، همه داکیومنت به عنوان محتوا در نظر گرفته می‌شود و یک شناسه دیفالت تعیین می‌شود\n",
        "#         doc_id, doc_content = \"Default_ID\", doc.strip()\n",
        "\n",
        "#     # محاسبه tf و idf برای داکیومنت فعلی\n",
        "#     tfidf_matrix, terms = calculate_tf_idf([doc_content])\n",
        "#     # تبدیل tf-idf به دیتافریم\n",
        "#     df_tf = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
        "#     # اضافه کردن شناسه داکیومنت به df_tf\n",
        "#     df_tf.insert(0, \"Document ID\", doc_id)\n",
        "#     # اضافه کردن df_tf به لیست tf ها\n",
        "#     dfs_tf.append(df_tf)\n",
        "\n",
        "# # ترکیب تمام df_tf ها به یک df_tf\n",
        "# df_tf_combined = pd.concat(dfs_tf, ignore_index=True)\n",
        "\n",
        "# # ذخیره DataFrame در فایل Excel\n",
        "# df_tf_combined.to_excel(\"tf_results_combined.xlsx\", index=False)\n",
        "\n",
        "# print(\"Results saved to tf_results_combined.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ31TwMpDBA7",
        "outputId": "9a65d993-5793-4b58-bab6-1a902dd60906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_results_combined.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.stem import SnowballStemmer\n",
        "# import os\n",
        "# import re\n",
        "\n",
        "# # مسیر فایل دیتاست CISI\n",
        "# cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# # لیستی از stop words از کتابخانه nltk\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # تابعی برای حذف stop words و stemming از متن\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenize متن به کلمات\n",
        "#     tokens = word_tokenize(text)\n",
        "#     # حذف stop words\n",
        "#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "#     # Stemming کلمات\n",
        "#     stemmer = SnowballStemmer(\"english\")\n",
        "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "#     return \" \".join(stemmed_tokens)\n",
        "\n",
        "# # تابعی برای محاسبه tf و idf برای هر داکیومنت\n",
        "# def calculate_tf_idf(documents):\n",
        "#     # تبدیل متن به ماتریس تعداد کلمات (Term Frequency)\n",
        "#     count_vectorizer = CountVectorizer(preprocessor=preprocess_text)\n",
        "#     term_counts = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "#     # محاسبه مقدار tf-idf\n",
        "#     tfidf_transformer = TfidfTransformer()\n",
        "#     tfidf_matrix = tfidf_transformer.fit_transform(term_counts)\n",
        "\n",
        "#     return tfidf_matrix, count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# # خواندن محتوای فایل CISI\n",
        "# with open(cisi_file, \"r\") as f:\n",
        "#     cisi_content = f.read()\n",
        "\n",
        "# # جدا کردن داکیومنت‌ها بر اساس I هر داکیومنت\n",
        "# documents = cisi_content.split(\".I\")[1:]\n",
        "\n",
        "# # لیستی برای ذخیره tf-idf هر داکیومنت\n",
        "# dfs_tf = []\n",
        "\n",
        "# # محاسبه tf و idf برای هر داکیومنت جداگانه\n",
        "# for doc in documents:\n",
        "#     # شناسایی الگوی .T در داکیومنت\n",
        "#     pattern = re.compile(r\"\\.T\", re.IGNORECASE)\n",
        "#     match = pattern.search(doc)\n",
        "\n",
        "#     if match:\n",
        "#         # اگر الگو پیدا شود، دو بخش جداگانه از داکیومنت استخراج می‌شود\n",
        "#         doc_id, doc_content = doc.split(\".T\")[0], doc.split(\".T\")[1].strip()\n",
        "#     else:\n",
        "#         # اگر الگو پیدا نشود، همه داکیومنت به عنوان محتوا در نظر گرفته می‌شود و یک شناسه دیفالت تعیین می‌شود\n",
        "#         doc_id, doc_content = \"Default_ID\", doc.strip()\n",
        "\n",
        "#     # محاسبه tf و idf برای داکیومنت فعلی\n",
        "#     tfidf_matrix, terms = calculate_tf_idf([doc_content])\n",
        "#     # تبدیل tf-idf به دیتافریم\n",
        "#     df_tf = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
        "#     # اضافه کردن شناسه داکیومنت به df_tf\n",
        "#     df_tf.insert(0, \"Document ID\", doc_id)\n",
        "#     # اضافه کردن df_tf به لیست tf ها\n",
        "#     dfs_tf.append(df_tf)\n",
        "\n",
        "# # ترکیب تمام df_tf ها به یک df_tf\n",
        "# df_tf_combined = pd.concat(dfs_tf, ignore_index=True)\n",
        "\n",
        "# # ذخیره DataFrame در فایل Excel\n",
        "# df_tf_combined.to_excel(\"tf_results_combined.xlsx\", index=False)\n",
        "\n",
        "# print(\"Results saved to tf_results_combined.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7g4vxjDidpn",
        "outputId": "8ba22e56-4087-449e-d257-3463b13dd639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_results_combined.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import os\n",
        "import re\n",
        "\n",
        "# مسیر فایل دیتاست CISI\n",
        "cisi_file = \"CISI.ALL\"\n",
        "\n",
        "# لیستی از stop words از کتابخانه nltk\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# تابعی برای حذف stop words و stemming از متن\n",
        "def preprocess_text(text):\n",
        "    # Tokenize متن به کلمات\n",
        "    tokens = word_tokenize(text)\n",
        "    # حذف stop words\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Stemming کلمات\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "# تابعی برای محاسبه tf و idf برای هر داکیومنت\n",
        "def calculate_tf_idf(documents):\n",
        "    # تبدیل متن به ماتریس تعداد کلمات (Term Frequency)\n",
        "    count_vectorizer = CountVectorizer(preprocessor=preprocess_text)\n",
        "    term_counts = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "    # محاسبه مقدار tf-idf\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    tfidf_matrix = tfidf_transformer.fit_transform(term_counts)\n",
        "\n",
        "    return tfidf_matrix, count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# خواندن محتوای فایل CISI\n",
        "with open(cisi_file, \"r\") as f:\n",
        "    cisi_content = f.read()\n",
        "\n",
        "# جدا کردن داکیومنت‌ها بر اساس I هر داکیومنت\n",
        "documents = cisi_content.split(\".I\")[1:]\n",
        "\n",
        "# لیستی برای ذخیره tf-idf هر داکیومنت\n",
        "dfs_tf = []\n",
        "\n",
        "# محاسبه tf و idf برای هر داکیومنت جداگانه\n",
        "for doc in documents:\n",
        "    # شناسایی الگوی .T در داکیومنت\n",
        "    pattern = re.compile(r\"\\.T\", re.IGNORECASE)\n",
        "    match = pattern.search(doc)\n",
        "\n",
        "    if match:\n",
        "        # اگر الگو پیدا شود، دو بخش جداگانه از داکیومنت استخراج می‌شود\n",
        "        doc_id, doc_content = doc.split(\".T\")[0], doc.split(\".T\")[1].strip()\n",
        "    else:\n",
        "        # اگر الگو پیدا نشود، همه داکیومنت به عنوان محتوا در نظر گرفته می‌شود و یک شناسه دیفالت تعیین می‌شود\n",
        "        doc_id, doc_content = \"Default_ID\", doc.strip()\n",
        "\n",
        "    # شناسایی قسمت‌های W و X\n",
        "    pattern_w = re.compile(r\"\\.W\", re.IGNORECASE)\n",
        "    pattern_x = re.compile(r\"\\.X\", re.IGNORECASE)\n",
        "    match_w = pattern_w.search(doc_content)\n",
        "    match_x = pattern_x.search(doc_content)\n",
        "\n",
        "    if match_w and match_x:\n",
        "        # اگر الگوهای متناظر با W و X پیدا شوند، بخش مورد نظر از داکیومنت استخراج می‌شود\n",
        "        doc_content = doc_content[match_w.end():match_x.start()]\n",
        "\n",
        "    # محاسبه tf و idf برای داکیومنت فعلی\n",
        "    tfidf_matrix, terms = calculate_tf_idf([doc_content])\n",
        "    # تبدیل tf-idf به دیتافریم\n",
        "    df_tf = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
        "    # اضافه کردن شناسه داکیومنت به df_tf\n",
        "    df_tf.insert(0, \"Document ID\", doc_id)\n",
        "    # اضافه کردن df_tf به لیست tf ها\n",
        "    dfs_tf.append(df_tf)\n",
        "\n",
        "# ترکیب تمام df_tf ها به یک df_tf\n",
        "df_tf_combined = pd.concat(dfs_tf, ignore_index=True)\n",
        "\n",
        "# ذخیره DataFrame در فایل Excel\n",
        "df_tf_combined.to_excel(\"tf_results_combined.xlsx\", index=False)\n",
        "\n",
        "print(\"Results saved to tf_results_combined.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrXHgcyjq_nq",
        "outputId": "8d380e63-2b6f-4a46-aef5-53a25d0c4bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to tf_results_combined.xlsx\n"
          ]
        }
      ]
    }
  ]
}